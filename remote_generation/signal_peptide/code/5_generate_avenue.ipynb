{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "from translator import SignalTranslator\n",
    "\n",
    "with open('../outputs/ctable_token.pkl', 'rb') as f:\n",
    "    ctable = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for tokenizing new inputs\n",
    "alphabet = ' .$ACDEFGHIKLMNPQRSTUVWXYZ'\n",
    "max_len_in = 107 # max length of prot seq (105 aa) + 2 for tokens\n",
    "max_len_out = 72\n",
    "n_chars = len(alphabet)\n",
    "\n",
    "with open('../data/ctable_copies/ctable_token_master.pkl', 'rb') as f:\n",
    "    ctable = pickle.load(f)\n",
    "\n",
    "def encode(seqs, max_len, ctable):\n",
    "    if ctable.one_hot:\n",
    "        X = np.zeros((len(seqs), max_len, n_chars))\n",
    "    else:\n",
    "        X = np.zeros((len(seqs), max_len))\n",
    "    seqs = ['$' + seq + '.' for seq in seqs]\n",
    "    seqs = [seq + ' ' * ((max_len) - len(seq))for seq in seqs]\n",
    "    for i, seq in enumerate(seqs):\n",
    "        X[i] = ctable.encode(seq, max_len)\n",
    "    return X\n",
    "\n",
    "def to_h5py(seqs, fname, ctable):\n",
    "    chunksize = 500\n",
    "    with h5py.File(fname, 'w') as f:\n",
    "        if ctable.one_hot:\n",
    "            print('true')\n",
    "            X = f.create_dataset('X', (len(seqs), max_len_in, n_chars))\n",
    "        else:\n",
    "            X = f.create_dataset('X', (len(seqs), max_len_in))          \n",
    "        for i in range(0, len(seqs), chunksize):\n",
    "            X[i:i + chunksize, :] = encode([seq for seq in seqs[i:i+chunksize]], max_len_in, ctable)\n",
    "        left = len(seqs) % chunksize\n",
    "        if left > 0:\n",
    "            X[-left:, :] = encode([seq for seq in seqs[-left:]], max_len_in, ctable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_signal_peptide(df):\n",
    "    def get_sp_length(signal_peptide):\n",
    "        match = re.search(r'\\d+\\.\\.(\\d+)', signal_peptide)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    signal_col = \"Signal peptide\"\n",
    "    seq_col = \"Sequence\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"Sequence\"] = df_copy.apply(lambda x: x[seq_col][get_sp_length(x[signal_col]):], axis = 1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sps(inputs: pd.DataFrame, clf: SignalTranslator, beam_size):\n",
    "    test_seqs = pd.Series(inputs['Sequence']).values\n",
    "    test_seqs = [s[:100] for s in test_seqs]\n",
    "    test_filename = ('../data/temp_tokens.hdf5')\n",
    "    to_h5py(test_seqs, test_filename, ctable)\n",
    "\n",
    "    file = h5py.File(test_filename)\n",
    "    training_data = SignalTranslator.generator_from_h5_noy(file, 64, shuffle=False, use_cuda=False)\n",
    "    src = next(training_data) # src is prot sequence, tgt is signal pep\n",
    "    file.close()\n",
    "    clf_outputs  = clf.translate_batch(src, beam=beam_size)\n",
    "    decoded, all_hyp, all_scores, enc_outputs, dec_outputs, enc_slf_attns, dec_slf_attns, dec_enc_attn = clf_outputs\n",
    "\n",
    "    results = pd.DataFrame({\"source\": [ctable.decode(s.data.cpu().numpy()) for s in src[0]],\n",
    "                            \"sp\": decoded})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cuda=False, d_inner_hid=1100, d_k=64, d_model=550, d_v=64, d_word_vec=550, dropout=0.1, embs_share_weight=True, max_token_seq_len=107, n_head=5, n_layers=6, proj_share_weight=True, src_vocab_size=27, tgt_vocab_size=27) Namespace(beam_size=1, ctable=<tools.CharacterTable object at 0x7fc1a0886160>, max_trans_length=72, n_best=1) Namespace(d_model=None, decay_power=-0.03, lr_max=0.0001, n_warmup_steps=12500, optim=<class 'torch.optim.adam.Adam'>)\n",
      "position_encoding\n",
      "position_encoding\n",
      "Initiated Transformer with 27403200 parameters.\n"
     ]
    }
   ],
   "source": [
    "# Load a Model Checkpoint\n",
    "chkpt_name = 'SIM99_550_12500_64_6_5_0.1_64_100_0.0001_-0.03_99'\n",
    "chkpt = \"../outputs/models/model_checkpoints/\" + chkpt_name + \".chkpt\"\n",
    "clf = SignalTranslator.load_model(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input proteins and remove existing signal peptides\n",
    "df = pd.read_csv(\"../data/pharmaceutical_proteins.tsv\", sep=\"\\t\")\n",
    "df = remove_signal_peptide(df)\n",
    "\n",
    "with open(\"../data/mature_sequences.txt\") as f:\n",
    "    seqs = [line.strip() for line in f.readlines() if not line.startswith(\">\")]\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame({\"Sequence\": seqs})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/miniconda3/envs/pytorch_p36_clean_cuda/lib/python3.6/site-packages/torch/nn/modules/module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n",
      "/home/anton/SPGen/remote_generation/signal_peptide/code/translator.py:377: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.model.prob_projection(dec_output)\n"
     ]
    }
   ],
   "source": [
    "# Generate SPs for all proteins combined\n",
    "full_sps = generate_sps(df, clf=clf, beam_size=10)\n",
    "\n",
    "# Generate SPs for all proteins separately\n",
    "individual_sps = pd.concat(df.apply(lambda row: generate_sps(row, clf, beam_size=10), axis=1).to_list(), ignore_index=True)\n",
    "\n",
    "# Generate SPs for random combinations of proteins\n",
    "random_combn_sps = pd.concat([\n",
    "    generate_sps(df.sample(n=15, random_state=i), clf, beam_size=5)\n",
    "    for i in range(20)\n",
    "])\n",
    "random_combn_sps_smol = pd.concat([\n",
    "    generate_sps(df.sample(n=5, random_state=i), clf, beam_size=5)\n",
    "    for i in range(100)\n",
    "])\n",
    "\n",
    "\n",
    "all_sps = pd.concat([full_sps, individual_sps, random_combn_sps, random_combn_sps_smol]).drop_duplicates(subset=\"sp\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sps.to_csv(\"../outputs/results/generated_sps.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
